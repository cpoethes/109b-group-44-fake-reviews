{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone 2: Acquire and Understand the Data\n",
    "\n",
    "Project Group Members:\n",
    "- Geoffrey Humphreys\n",
    "- Amir Koupaei\n",
    "- Chris Moon\n",
    "- Connor Poetzinger\n",
    "\n",
    "Wednesday, March 27, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code and analysis for the second milestone of our project. The objective of this milestone focuses on preparing the [data set](https://www.kaggle.com/datasets/mexwell/fake-reviews-dataset) for subsequent analysis. For submission, follow the Milestone Checklist below.\n",
    "\n",
    "**Milestone Checklist:**\n",
    "- Access: Download, collect, or scrape* the dataset from the relevant source(s).\n",
    "  - **Achieved by downloading the dataset from Kaggle.**\n",
    "- Load: Start a new Jupyter Notebook, import necessary Python libraries and load your data set for inspection.\n",
    "  - **Achieved by using the `pandas` library to load the dataset.**\n",
    "- Understand: Examine the dataset. Ensure you understand the different features and their data types.\n",
    "  - **Achieved by previewing the data, examining summary statistics, data types, missing values, and more**\n",
    "- Preprocessing: Document any cleaning or preprocessing setup that may be necessary/required. This portion only includes the preprocessing steps, not the actual execution of the steps.\n",
    "  - **Achieved by identifying and documenting the preprocessing steps necessary for the dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set Information and Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our group project, we are focusing on the critical task of distinguishing between genuine and counterfeit product reviews leveraging a specially curated dataset designed to mirror the complexities and nuances found in real-world online review platforms. This dataset contains a balanced collection of 40,000 product reviews, equally divided into two distinct categories:\n",
    "- **Original Reviews**: Genuine product reviews written by real customers.\n",
    "**Computer-Generated Fake Reviews**: Counterfeit product reviews generated by an algorithm.\n",
    "\n",
    "Each review in the dataset is annotated according to its source category (OR or GC), enabling us to train and evaluate machine learning models to classify reviews as genuine or counterfeit. The dataset is stored in a CSV file, with each row representing a single review and containing the following columns:\n",
    "1. `category`: Product category\n",
    "2. `rating`: Rating of the product\n",
    "3. `label`: Label indicating whether the review is fake or real\n",
    "4. `text`: Review text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is available in a CSV file named `fake_reviews.csv`. We will load the data into a pandas DataFrame and examine the first few rows to understand the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/raw/fake reviews dataset.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Rows:** 40,432\n",
    "- **Columns:** 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By previewing the data, we can see there are reviews for various product categories with a rating between 1 and 5. The `text_` column contains the review text with messy data that will require preprocessing to clean and prepare for analysis. For this type of task (cleaning unstructured text data), we will need to use natural language processing (NLP) techniques to process the text data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lucky for us, the dataset contains no missing values, its classes are balanced, and the data types are easy to work with. We can focus on cleaning the text data and preparing it for analysis. The independent structured data columns (`category`, `rating`) will be useful for exploratory data analysis (EDA) and feature engineering. For modeling, we should consider One Hot encoding the `category` column and standardizing the `rating` column. The dependent variable `label` will be Label Encoded to prepare for classification modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the preprocessing steps necessary for the dataset. SKlearn's custom transformers will be used to implement these steps in the subsequent milestone. The preprocessing steps include:\n",
    "1. **Text Cleaning**: Remove special characters, punctuation, stopwords, and perform lemmatization. The package `nltk` will be used for this task.\n",
    "2. **One-Hot Encoding**: Encode the `category` column using one-hot encoding.\n",
    "3. **Standardization**: Standardize the `rating` column to ensure all features are on the same scale.\n",
    "4. **Label Encoding**: Encode the `label` column to convert the target variable into numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleanerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        # Using pandarallel for parallel processing\n",
    "        X_copy[self.column_name] = X_copy[self.column_name].parallel_apply(\n",
    "            self.clean_text\n",
    "        )\n",
    "        return X_copy\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
    "        text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)  # Remove URLs\n",
    "        text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Keep only alphabets\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        tokens = word_tokenize(text)  # Tokenization\n",
    "        tokens = [\n",
    "            word for word in tokens if word.lower() not in stopwords.words(\"english\")\n",
    "        ]  # Remove stopwords\n",
    "        return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "        self.encoder = OneHotEncoder()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.encoder.fit(X[[self.column_name]])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        encoded = self.encoder.transform(X[[self.column_name]]).toarray()\n",
    "        for i, category in enumerate(self.encoder.categories_[0]):\n",
    "            X_copy[category] = encoded[:, i]\n",
    "        X_copy.drop(columns=[self.column_name], inplace=True)\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScalerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X[[self.column_name]])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        X_copy[self.column_name] = self.scaler.transform(X[[self.column_name]])\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.label_encoder.fit(X[self.column_name])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        X_copy[self.column_name] = 1 - self.label_encoder.transform(X[self.column_name])\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformations(\n",
    "    df,\n",
    "    text_clean=True,\n",
    "    one_hot_encode=True,\n",
    "    standardize=True,\n",
    "    encode_label=True,\n",
    "    text_column=None,\n",
    "    category_column=None,\n",
    "    numerical_column=None,\n",
    "    label_column=None,\n",
    "):\n",
    "    if text_clean and text_column:\n",
    "        text_cleaner = TextCleanerTransformer(column_name=text_column)\n",
    "        df = text_cleaner.transform(df)\n",
    "\n",
    "    if one_hot_encode and category_column:\n",
    "        one_hot_encoder = OneHotEncoderTransformer(column_name=category_column)\n",
    "        one_hot_encoder.fit(df)\n",
    "        df = one_hot_encoder.transform(df)\n",
    "\n",
    "    if standardize and numerical_column:\n",
    "        scaler = StandardScalerTransformer(column_name=numerical_column)\n",
    "        scaler.fit(df)\n",
    "        df = scaler.transform(df)\n",
    "\n",
    "    if encode_label and label_column:\n",
    "        label_encoder = LabelEncoderTransformer(column_name=label_column)\n",
    "        label_encoder.fit(df)\n",
    "        df = label_encoder.transform(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations\n",
    "transformed_df = apply_transformations(\n",
    "    df,\n",
    "    text_column=\"text_\",\n",
    "    category_column=\"category\",\n",
    "    numerical_column=\"rating\",\n",
    "    label_column=\"label\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.to_csv(\"../../data/processed/processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this milestone of our project, our team successfully acquired and prepared our dataset for subsequent analysis. The data was sourced from Kaggle, obtaining a collection of over forty thousand product reviews divided into Original Reviews(OR) and Computer-Generated Fake Reviews(GC). Each review was accompanied by essential features such as product category, rating, label (indicating whether it's genuine or fake), and the review text itself. Upon loading the data into a pandas DataFrame, the first priority was to gain a deep understanding of the structure and content.\n",
    "\n",
    "Through examination, it was confirmed that the integrity of the dataset was intact, with no missing values, balanced classes, and straightforward data types. The data was then preprocessed to clean the text data, one-hot encode the `category` column, standardize the `rating` column, and label encode the `label` column. These preprocessing steps will be implemented in the subsequent milestone using custom transformers from the `sklearn` library. Custom transformers will allow us to streamline the preprocessing steps and fit into a machine learning pipeline for model training and evaluation.\n",
    "\n",
    "The next milestone will focus on exploratory data analysis (EDA) to gain insights into the data and feature engineering to create new features that may improve model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
